General Instruction:
Hidden Markov Models (HMMs) are powerful statistical models for modeling sequential or time-series data, and have been successfully used in many tasks such as speech recognition, protein/DNA sequence analysis, robot control, and information extraction on text data. The purpose of this assignment is to familiarize you with the underlying statistical model by implementing an HMM and its associated algorithms for the task of Part-of-Speech (POS) tagging. Specifically, you will tackle the following three fundamental problems in HMMs:
• Evaluation: Computing the probability of an observed sequence of symbols under a particular HMM, by implementing the Forward and the Backward algorithms.
• Decoding: Finding the most likely state transition path associated with an observed sequence by implementing the Viterbi algorithm.
• Learning: Adjusting the parameters of an HMM model to maximize the likelihood of some observed data under a model by implementing the Baum-Welch re-estimation algorithm.

POS tagging is a central task in Natural Language Processing that is useful for a wide variety of appli- cations. A POS tag is a label such as “Noun” or “Verb” that is assigned to words based on their function in a context. For example, the word “ate” in the sentence “I ate a pizza” is a verb. What makes the problem of POS tagging interesting is that words are often ambiguous and can be assigned different labels based on their context. For example the same word “bank” is a noun in the sentence “I went to the bank”, while it is a verb in the sentence “I bank on you”.

Data and files:
• hmm-trans.txt,hmm-emit.txtandhmm-prior.txt
These files contain pre-trained model parameters of an HMM that you will use in testing your implementation of the Evaluation and Decoding problems. The format of the first two files are analogous and is as follows. Every line1 in these files consists of a conditional probability distribution. In the case of transition probabilities, this distribution corresponds to the probability of transitioning into another state, given a current state. Similarly, in the case of emission probabilities, this distribution corresponds to the probability of emitting a particular symbol, given a current state. For example, every line in hmm-trans.txt has the following format:
     <Curr-State> <Nxt-State0>:<Prob-Val0> ...  <Nxt-StateN>:<Prob-ValN>
The format of hmm-prior.txt is slightly different and only contains a single probability distribution over starting states. Each line contains the name of a state and its associated starting probability value, like so: <State0> <Prob-Val0>.
• train.txtanddev.txt
These files contains plain text data that you will use in testing your implementation of the Evaluation, Decoding and Learning problems. Specifically the text contains one sentence per line that has already been pre-processed, cleaned and tokenized. You do not need to perform any processing of any kind. You should treat every line as a separate sequence and assume that it has the following format:
<Word0> <Word1> ... <WordN>
where every <WordK> unit token is whitespace separated. Note that dev.txt is the plain text
version of dev-tag.txt.
• dev-tag.txt
This file contains labelled data that you will use to debug your implementation of the Decod- ing problem. The labels are not gold standard but are generated by running our decoder on dev.txt. Specifically the text contains one sentence per line that has already been preprocessed, cleaned and tokenized. You should treat every line as a separate sequence and assume that it has the following format:
     <Word0>_<Tag0> <Word1>_<Tag1> ...  <WordN>_<TagN>
where every <WordK>_<TagK> unit token is whitespace separated. • dev-probs.txt
This file contains log-probability values that you will use to debug your implementation of the Evaluation problem. Every line in this file is a log-probability value assigned to a sentence in dev.txt, under the HMM parameters in hmm-trans.txt, hmm-emit.txt and hmm-prior.txt. Specifically, the K th line in dev-probs.txt corresponds to the log-probability of the K th sen- tence in dev.txt under the model parameters.
• eval.py
This is an evaluation script that you can use to test your output from decoding against ref- erence POS tagged text. It will be useful in debugging and testing your implementation of the Decoding problem. The command line signature for calling the script is as follows: $ python eval.py <ref-file> <sys-file>, where <ref-file> is some POS-tagged reference file, and <sys-file> is your system generated hypothesis. Both files need to be in the format of dev-tag.txt. An evaluation score is assigned to <sys-file> based on the number of sentences tagged correctly with respect to <ref-file>.
• logsum.py
This script contains a function that allows summing of exponentiated logarithmic numbers to a very high degree of precision. It will be useful in your implementation of the Evaluation, Decoding and Learning tasks, with regards to the problem of numerical underflow you will likely face (see section 7 for further details). While the sample function is in Python, equivalent Java code should be fairly easy to implement. For example, check out the Java.lang.Math.log1p() method, which is a necessary building block for this function.

Implementation:
1. EVALUATION — THE FORWARD AND BACKWARD ALGORITHMS
Implement the Forward and Backward Algorithms in two different files named alpha.py and beta.py respectively. Both scripts should expect the same command-line arguments and print their outputs to std-out in the same format. The command-line signature of alpha.py is given as an example:
$ python alpha.py <dev> <hmm-trans> <hmm-emit> <hmm-prior>

2. DECODING — THE VITERBI ALGORITHM
The Viterbi algorithm is a dynamic programming algorithm that computes the most likely state tran- sition path given an observed sequence of symbols.

Implement the Viterbi algorithm in a file named viterbi.py. The command-line signature of this script should be as follows:
$ python viterbi.py <dev> <hmm-trans> <hmm-emit> <hmm-prior>
    
3. LEARNING — THE BAUM-WELCH ALGORITHM
Note that while the Evaluation and Decoding problems have treated each sentence as an isolated input, in training we generally want to treat the dataset in aggregate form. In other words, we want to maximize the likelihood of the entire dataset.

Implement the Baum-Welch re-estimation algorithm in a file named baumwelch.py. The command-line signature of this script should be as follows:
$ python baumwelch.py <train> [<hmm-trans> <hmm-emit> <hmm-prior>]